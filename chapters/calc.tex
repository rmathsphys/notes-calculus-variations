%!TEX root = ../main.tex
\section{Calculus in $\texorpdfstring{\R^n}{Rn}$}

\subsection{Critical points and extreme values}
Throughout this section let $f : D \subseteq \R^n \to \R$ be a function.

\begin{ndfn}[Global minimum]
  The function $f$ has a global minimum at $a \in D$ if $f(a) \leq f(x)$, for all $x \in D$.
\end{ndfn}
\begin{ndfn}[Local minimum]
  The function $f$ has a local minimum at $a \in D$ if there is an open set $U \subseteq D$, with $a \in U$, such that $f(a) \leq f(x)$, for all $x \in U$.
\end{ndfn}
\begin{ndfn}[Maximum]
  The function $f$ has a local (global) maximum at $a \in D$, if $-f$ has a local (global) minimum at $a$.
\end{ndfn}
We use the word \emph{extremum} to mean either the maximum or the minimum.

Global extrema might not exist, and, in general, it is difficult to find them. Instead, we focus mainly on the local extrema.
\begin{nprop}
  If $f$ has a global minimum (maximum) at $a \in D$, then it has a local minimum (maximum) at $a$.
\end{nprop}

\begin{ndfn}[Critical points]
  Let $f$ be at least $C^1$. We say that $a \in D$ is a critical point of $f$, if $f'(a) = 0$.
\end{ndfn}
Note that we have not defined critical points for functions that are not differentiable, or whose first derivative is not continuous. Similarly, we have ignored the complications due to boundary points.
\begin{nthm}
\label{thm:extremum-critical}
  Let $f$ be at least $C^1$. If $f$ has an extremum at $a \in D$, then $a$ is a critical point of $f$; that is $f'(a)=0$.
\end{nthm}

\subsection{Convex functions}
The converse of theorem~(\ref{thm:extremum-critical}) is not true in general. For example, a critical point can be a point of inflection or a saddle point, rather than being an extremum. If the function satisfies some additional properties then we can obtain more precise information about the extrema by studying only the critical points.
\begin{ndfn}[Convex function]
  A function $f:D \to \R$ is called convex if $D$ is a convex set, and $\forall x, y \in D$ and $\forall t \in (0,1)$,
  \begin{equation}
    (1-t) f(x) + t f(y) \geq f\left((1-t)x + ty\right).
  \end{equation}
  If the inequality is strict for all $t \in (0,1)$, then the function is called strictly convex.
\end{ndfn}
Convex functions have particularly nice properties for optimisation problems. The following results discuss some of these features.
\begin{nprop}
  Suppose $f$ is convex, and that it has an extremum at $a \in D$. Then, $f$ has a global minimum at $a \in D$.
\end{nprop}
\begin{nprop}
  Suppose $f \in C^{1}$ is convex, and that $f'(a)=0$ for some $a \in U$. Then, $f'(x) \neq 0$ for all $x \neq a$. In other words, if there is a critical point then it is unique.
\end{nprop}

EXAMPLE: F NOT CONVEX, LOCAL MIN IS NOT GLOBAL MIN.


\subsection{Fundamental Theorem of Calculus of Variations}
Before proceeding with the analysis of variational problems, we will study a very important result which is central in the derivation of the Euler-Lagrange equations.

Consider a continuous function $f : [x_1, x_2] \to \R$ such that
\begin{equation}
  \label{eq:ftcv-motivation}
  \int_{x_1}^{x_2} f(x) h(x) dx = 0
\end{equation}
for all the functions $h \in C^{0}([x_1, x_2])$. Does this imply that $f(x)=0$ for all $x \in [x_1, x_2]$? By taking $h = f$, equation~\eqref{eq:ftcv-motivation} becomes $\int_{x_1}^{x_2} f(x)^2 dx = 0$. This is the integral of a non-negative continuous function, and therefore, we obtain that $f(x)=0$ for all $x \in [x_1, x_2]$ in this case. However, what happens when the `test functions' $h(x)$ are restricted to a smaller set, such as $C^{2}$, or when there are boundary conditions on $h(x)$?

The next result, known as the fundamental theorem of calculus of variations (FTCV), presents this generalisation.
\begin{nthm}[FTCV version I]
  \label{thm:ftcv}
  Let $x_1 < x_2$ and $k \in \N$. If $f : [x_1, x_2] \to \R$ is a continuous function such that
  \begin{equation}
    \label{eq:ftcv}
    \int_{x_1}^{x_2} f(x) h(x) dx = 0
  \end{equation}
  for every function $h \in C^{k}([x_1, x_2])$ with $h(x_1)=h(x_2)=0$, then $f(x)=0$ for all $x \in [x_1, x_2]$.
\end{nthm}
\begin{proof}
  TBC.
\end{proof}

\begin{nex}
  Prove that the function $h_0$ given above is $C^{k}$.
\end{nex}

The test functions $h$ can also be restricted to the set $C^{\infty}$. The theorem still holds, and the proof remains exactly the same, but we need to chance $h_0$ so that it is infinitely differentiable. One one choice is
\begin{equation}
  h_0(x) = \begin{cases}\exp(-1/(1-x^2)), & x \in (-1,1)\\ 0, &\text{otherwise}\end{cases}
\end{equation}
This function can also be used in the previous proof.

There are several versions of FTCV, with slight modifications. The version presented in theorem~(\ref{thm:ftcv}) will be sufficient for us. For reference, a more general result is given below.
\begin{nthm}[FTCV version II]
  \label{thm:ftcv-supp}
  If $f : [x_1, x_2] \to \R$ is a continuous function such that
  \begin{equation}
    \int_{x_1}^{x_2} f(x) h(x) dx = 0
  \end{equation}
  for every function $h \in C^{\infty}([x_1, x_2])$ with $\supp h \subset (x_1, x_2)$, then $f(x)=0$ for all $x \in [x_1, x_2]$.
\end{nthm}
This version of FTCV implies theorem~(\ref{thm:ftcv}), because it is based on a strictly smaller set of assumptions.
